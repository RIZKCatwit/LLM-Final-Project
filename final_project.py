# -*- coding: utf-8 -*-
"""Final Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Uz7Vas8T5a93PMZblwiixr56XoypiOwb
"""

import pandas as pd

df = pd.read_csv('movies_metadata.csv')

print(df.head())

# Print the list of column names
print("Columns in the dataset:")
print(df.columns.tolist())

maindf = df[['overview', 'genres', 'title']]
print(maindf.head)

print("\nDataset information:")
print(maindf.info())

# Drop rows with missing values in either 'overview' or 'title'
df_cleaned = maindf.dropna(subset=['overview', 'title', 'genres'])

# Verify the result by checking the DataFrame information
print("After dropping rows with missing 'overview' or 'title':")
print(df_cleaned.info())

df_train = df_cleaned[['overview', 'genres', 'title']]
print(df_train.info())

"""Two datasets,

df_cleaned = 'overview', 'title', 'genres'

df_train = 'overview', 'title'

* START THE RUN FROM HERE AFTER LOADING THE DATA SET*
"""

import pandas as pd

data = pd.read_csv('train_movies_metadata.csv')

import pandas as pd

evaldata = pd.read_csv('cleaned_movies_metadata.csv')

"""tokenizer : LAMA2

"""

!pip install datasets

import pandas as pd
import torch
import numpy as np
from transformers import LlamaForCausalLM, LlamaTokenizer, Trainer, TrainingArguments
from peft import get_peft_model, LoraConfig, TaskType
from datasets import Dataset

# Load data
train_data = pd.read_csv('/content/train_movies_metadata.csv')
eval_data = pd.read_csv('/content/train_movies_metadata.csv')

# Quick check
print(train_data.head())

import pandas as pd
from sklearn.preprocessing import MultiLabelBinarizer

# Load data
data = pd.read_csv('train_movies_metadata.csv')
data = data[['overview', 'genres']].dropna()
data['genres'] = data['genres'].apply(lambda x: x.split('|'))

# Subsample data if needed (e.g., 10k examples)
data = data.sample(n=10000, random_state=42)

print(data.head())

# Create and fit the MultiLabelBinarizer
mlb = MultiLabelBinarizer()
genre_labels = mlb.fit_transform(data['genres'])

# Now you can split data and convert to Hugging Face Dataset...

import pandas as pd
import ast

# Load the CSV file
data = pd.read_csv('train_movies_metadata.csv')

def clean_genres(genre_str):
    """
    Convert a genre string into a list of genre names.
    Handles two cases:
      1. A pipe-separated string (e.g., "Action|Comedy|Drama")
      2. A string representation of a list of dictionaries (e.g.,
         "[{'id': 28, 'name': 'Action'}, {'id': 35, 'name': 'Comedy'}]")
    """
    if pd.isna(genre_str):
        return []
    # Case 1: Pipe-separated string
    if isinstance(genre_str, str) and '|' in genre_str:
        return [g.strip() for g in genre_str.split('|') if g.strip() != '']
    # Case 2: String representation of a list of dictionaries
    try:
        parsed = ast.literal_eval(genre_str)
        if isinstance(parsed, list):
            names = []
            for item in parsed:
                if isinstance(item, dict) and 'name' in item:
                    names.append(item['name'])
                elif isinstance(item, str):
                    names.append(item)
            return names
    except Exception:
        pass
    return []

# Create a new column 'cleaned_genres'
data['cleaned_genres'] = data['genres'].apply(clean_genres)

# Optionally, drop rows with no genres after cleaning
data = data[data['cleaned_genres'].map(lambda x: len(x)) > 0]

# Display the first 10 rows to verify the cleaning process
print(data[['overview', 'cleaned_genres']].head(10))
print("Number of movies after cleaning:", len(data))

import pandas as pd
import numpy as np
import ast
from sklearn.preprocessing import MultiLabelBinarizer
from sklearn.model_selection import train_test_split
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import torch
from datasets import Dataset
from sklearn.metrics import f1_score

# --- Data Loading & Cleaning ---
data = pd.read_csv('train_movies_metadata.csv')
data = data[['overview', 'genres']].dropna()

def clean_genres(genre_str):
    """
    Cleans a genre string by handling:
    1. Pipe-separated strings (e.g., "Action|Comedy|Drama")
    2. String representations of lists of dictionaries (e.g., "[{'id': 28, 'name': 'Action'}, ...]")
    Returns a list of genre names.
    """
    if pd.isna(genre_str):
        return []
    if isinstance(genre_str, str) and '|' in genre_str:
        return [g.strip() for g in genre_str.split('|') if g.strip()]
    try:
        parsed = ast.literal_eval(genre_str)
        if isinstance(parsed, list):
            names = []
            for item in parsed:
                if isinstance(item, dict) and 'name' in item:
                    names.append(item['name'])
                elif isinstance(item, str):
                    names.append(item)
            return names
    except Exception:
        pass
    return []

data['cleaned_genres'] = data['genres'].apply(clean_genres)
data = data[data['cleaned_genres'].map(lambda x: len(x)) > 0]
print(data[['overview', 'cleaned_genres']].head(10))
print("Number of movies after cleaning:", len(data))

# --- Label Encoding ---
mlb = MultiLabelBinarizer()
genre_labels = mlb.fit_transform(data['cleaned_genres']).astype(np.float32)

# --- Train/Validation Split ---
train_texts, val_texts, train_labels, val_labels = train_test_split(
    data['overview'].tolist(), genre_labels, test_size=0.2, random_state=42
)
train_dataset = Dataset.from_dict({'text': train_texts, 'labels': train_labels})
val_dataset = Dataset.from_dict({'text': val_texts, 'labels': val_labels})

# --- Model & Tokenizer Setup ---
# Switched to a larger model for improved performance
model_name = 'bert-base-uncased'
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(mlb.classes_),
    problem_type="multi_label_classification"
)

# --- Tokenization ---
def tokenize(batch):
    return tokenizer(batch['text'], padding="max_length", truncation=True, max_length=256)

train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)

train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])
val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])

# --- Compute pos_weight for Weighted Loss ---
train_labels_np = np.array(train_dataset["labels"])  # shape: (num_samples, num_labels)
positive_counts = np.sum(train_labels_np, axis=0)
negative_counts = train_labels_np.shape[0] - positive_counts
pos_weight = torch.tensor(negative_counts / (positive_counts + 1e-5), dtype=torch.float32)
pos_weight = torch.clamp(pos_weight, max=1000.0)
print("pos_weight (clipped):", pos_weight)

# --- Custom Trainer with Weighted Loss ---
class CustomTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.logits
        loss_fct = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight.to(model.device))
        loss = loss_fct(logits, labels)
        return (loss, outputs) if return_outputs else loss

# --- Training Arguments ---
training_args = TrainingArguments(
    output_dir='./movie-genre-model',
    eval_strategy='epoch',
    learning_rate=1e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_steps=10,
    save_total_limit=2,
    fp16=True,
    report_to='none'
)

trainer = CustomTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset
)

# --- Training ---
trainer.train()

# --- Evaluation with Dynamic Thresholding ---
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Get raw predictions from the validation set
pred_output = trainer.predict(val_dataset)
logits = pred_output.predictions
true_labels = np.array(val_dataset["labels"])

# Function to evaluate at a given threshold
def evaluate_threshold(threshold):
    preds = (torch.sigmoid(torch.tensor(logits)) > threshold).numpy().astype(np.int32)
    acc = accuracy_score(true_labels, preds)
    precision = precision_score(true_labels, preds, average="micro", zero_division=0)
    recall = recall_score(true_labels, preds, average="micro", zero_division=0)
    f1 = f1_score(true_labels, preds, average="micro", zero_division=0)
    return acc, precision, recall, f1

thresholds = np.linspace(0.1, 0.8, 8)
best_threshold = 0.5
best_f1 = 0.0

print("Threshold\tAccuracy\tPrecision\tRecall\t\tF1 Score")
for thresh in thresholds:
    acc, precision, recall, f1 = evaluate_threshold(thresh)
    print(f"{thresh:.2f}\t\t{acc:.4f}\t\t{precision:.4f}\t\t{recall:.4f}\t\t{f1:.4f}")
    if f1 > best_f1:
        best_f1 = f1
        best_threshold = thresh

print("\nOptimal Threshold:", best_threshold)
acc, precision, recall, f1 = evaluate_threshold(best_threshold)
print("Evaluation with optimal threshold", best_threshold)
print("Accuracy:", acc)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import multilabel_confusion_matrix

# Assume best_threshold and true_labels are defined from previous evaluation
# For example, using the optimal threshold:
best_threshold = 0.7  # from your evaluation above
preds_optimal = (torch.sigmoid(torch.tensor(logits)) > best_threshold).numpy().astype(np.int32)
true_labels = np.array(val_dataset["labels"])

# Compute a separate 2x2 confusion matrix for each label
mcm = multilabel_confusion_matrix(true_labels, preds_optimal)
num_labels = len(mlb.classes_)

# Plotting: if the number of labels is small, use a grid; otherwise, plot individually.
if num_labels <= 20:
    cols = 4
    rows = (num_labels // cols) + (num_labels % cols > 0)
    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))
    axes = axes.flatten()
    for idx, cm in enumerate(mcm):
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", ax=axes[idx])
        axes[idx].set_title(mlb.classes_[idx])
        axes[idx].set_xlabel("Predicted")
        axes[idx].set_ylabel("True")
    # Turn off any extra subplots
    for ax in axes[num_labels:]:
        ax.axis('off')
    plt.tight_layout()
    plt.show()
else:
    # If there are many labels, plot one by one
    for idx, cm in enumerate(mcm):
        plt.figure(figsize=(4,3))
        sns.heatmap(cm, annot=True, fmt="d", cmap="Blues")
        plt.title(f"Confusion Matrix for {mlb.classes_[idx]}")
        plt.xlabel("Predicted")
        plt.ylabel("True")
        plt.show()

model.save_pretrained("./movie-genre-model")
tokenizer.save_pretrained("./movie-genre-model")



